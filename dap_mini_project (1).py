# -*- coding: utf-8 -*-
"""DAP_MINI_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qudVSTbS5y2dfcO7BLjvonAI_pwsDzqj
"""

import pandas as pd
import itertools  #which provides the set of function which used to iterate over the large dataset in efficient way.This module is especially useful for handling combinations, permutations, and other advanced iteration tasks.
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score #(precision, recall, F1-score, and support),
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb        #Extreme Gradient Boosting.
from lightgbm import LGBMClassifier  #LightGBM stands for Light Gradient Boosting Machine.
import os   #to working with large datasets
import seaborn as sns
from wordcloud import WordCloud

df=pd.read_csv('/content/malicious_phish.csv')

print(df.shape)
df.head()

df.type.value_counts() # how many times tis appered in the dataset

df_phish = df[df.type=='phishing']
df_malware = df[df.type=='malware']
df_deface = df[df.type=='defacement']
df_benign = df[df.type=='benign']

phish_url = " ".join(i for i in df_phish.url)
wordcloud = WordCloud(width=800, height=500,colormap='Paired').generate(phish_url)
plt.figure( figsize=(8,10),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

malware_url = " ".join(i for i in df_malware.url)
wordcloud = WordCloud(width=800, height=500,colormap='Paired').generate(malware_url)
plt.figure( figsize=(8,10),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

deface_url = " ".join(i for i in df_deface.url)
wordcloud = WordCloud(width=800, height=500,colormap='Paired').generate(deface_url)
plt.figure( figsize=(8,10),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

benign_url = " ".join(i for i in df_benign.url)
wordcloud = WordCloud(width=800, height=500,colormap='Paired').generate(benign_url)
plt.figure( figsize=(8,10),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""Feature engineering"""

import re # to the regular expression
#Use of IP or not in domain
def having_ip_address(url):
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0
df['use_of_ip'] = df['url'].apply(lambda i: having_ip_address(i))

from urllib.parse import urlparse

def abnormal_url(url):
    hostname = urlparse(url).hostname
    hostname = str(hostname)
    match = re.search(hostname, url)
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0


df['abnormal_url'] = df['url'].apply(lambda i: abnormal_url(i)) #using each one of i one after the another

!pip install googlesearch-python

from googlesearch import search

def google_index(url):
    site = search(url, 5)
    return 1 if site else 0
df['google_index'] = df['url'].apply(lambda i: google_index(i))


def count_dot(url):
    count_dot = url.count('.')
    return count_dot

df['count.'] = df['url'].apply(lambda i: count_dot(i))
df.head()

def count_www(url):
    url.count('www')
    return url.count('www')

df['count-www'] = df['url'].apply(lambda i: count_www(i))

def count_atrate(url):

    return url.count('@')

df['count@'] = df['url'].apply(lambda i: count_atrate(i))


def no_of_dir(url):
    urldir = urlparse(url).path
    return urldir.count('/')

df['count_dir'] = df['url'].apply(lambda i: no_of_dir(i))

def no_of_embed(url):
    urldir = urlparse(url).path
    return urldir.count('//')

df['count_embed_domian'] = df['url'].apply(lambda i: no_of_embed(i))


def shortening_service(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                      'tr\.im|link\.zip\.net',
                      url)
    if match:
        return 1
    else:
        return 0
df['short_url'] = df['url'].apply(lambda i: shortening_service(i))

def count_https(url):
    return url.count('https')

df['count-https'] = df['url'].apply(lambda i : count_https(i))

def count_http(url):
    return url.count('http')

df['count-http'] = df['url'].apply(lambda i : count_http(i))

def count_per(url):
    return url.count('%')

df['count%'] = df['url'].apply(lambda i : count_per(i))

def count_ques(url):
    return url.count('?')

df['count?'] = df['url'].apply(lambda i: count_ques(i))

def count_hyphen(url):
    return url.count('-')

df['count-'] = df['url'].apply(lambda i: count_hyphen(i))

def count_equal(url):
    return url.count('=')

df['count='] = df['url'].apply(lambda i: count_equal(i))

def url_length(url):
    return len(str(url))


#Length of URL
df['url_length'] = df['url'].apply(lambda i: url_length(i))
#Hostname Length

def hostname_length(url):
    return len(urlparse(url).netloc)

df['hostname_length'] = df['url'].apply(lambda i: hostname_length(i))

df.head()

def suspicious_words(url):
    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',
                      url)
    if match:
        return 1
    else:
        return 0
df['sus_url'] = df['url'].apply(lambda i: suspicious_words(i))
def digit_count(url):
    digits = 0
    for i in url:
        if i.isnumeric():
            digits = digits + 1
    return digits


df['count-digits']= df['url'].apply(lambda i: digit_count(i))


def letter_count(url):   #checkin only the letter
    letters = 0
    for i in url:
        if i.isalpha():
            letters = letters + 1
    return letters


df['count-letters']= df['url'].apply(lambda i: letter_count(i))

df.head()

!pip install tld

#Importing dependencies
from urllib.parse import urlparse
from tld import get_tld
import os.path

#First Directory Length
def fd_length(url):
    urlpath= urlparse(url).path
    try:
        return len(urlpath.split('/')[1])
    except:
        return 0

df['fd_length'] = df['url'].apply(lambda i: fd_length(i))

#Length of Top Level Domain
df['tld'] = df['url'].apply(lambda i: get_tld(i,fail_silently=True))


def tld_length(tld):
    try:
        return len(tld)
    except:
        return -1

df['tld_length'] = df['tld'].apply(lambda i: tld_length(i))

df = df.drop("tld", axis=1)
df.columns  # This will display the remaining column names

"""EDA
1. Distribution of use_of_ip
"""

import seaborn as sns
sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df,hue="use_of_ip")

"""2. Distribution of abnormal url"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df,hue="abnormal_url")

"""3. Distribution of Google Index"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df,hue="google_index")

"""4. Distribution of Shorl URL"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df,hue="short_url")

"""5. Distribution of Suspicious URL"""

sns.set(style="darkgrid")
ax = sns.countplot(y="type", data=df,hue="sus_url")

"""6. Distribution of count of [.] dot

"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count.", kind="box", data=df)

"""
7. Distribution of count-www"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count-www", kind="box", data=df)

"""8. Distribution of count@"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count@", kind="box", data=df)

"""9. Distribution of count_dir"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="count_dir", kind="box", data=df)

"""10. Distribution of hostname length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="hostname_length", kind="box", data=df)

"""11. Distribution of first directory length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="fd_length", kind="box", data=df)

"""12. Distribution of top-level domain length"""

sns.set(style="darkgrid")
ax = sns.catplot(x="type", y="tld_length", kind="box", data=df)

"""Target Encoding"""

from sklearn.preprocessing import LabelEncoder

# Encoding categorical target variable
lb_make = LabelEncoder()
df["type_code"] = lb_make.fit_transform(df["type"])

"""Creation of Feature & Target"""

# Predictor Variables
X = df[['use_of_ip', 'abnormal_url', 'count.', 'count-www', 'count@',
       'count_dir', 'count_embed_domian', 'short_url', 'count-https',
       'count-http', 'count%', 'count?', 'count-', 'count=', 'url_length',
       'hostname_length', 'sus_url', 'fd_length', 'tld_length', 'count-digits',
       'count-letters']]

# Target Variable
y = df['type_code']

X.columns

"""Train Test Split"""

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=5)

"""Random forest classifier"""

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier        #You want a simple, stable model with less tuning

rf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print(classification_report(y_test, y_pred_rf, target_names=['benign', 'defacement', 'phishing', 'malware']))

score = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:   %0.3f" % score)

# Confusion Matrix for Random Forest
cm = confusion_matrix(y_test, y_pred_rf)
cm_df = pd.DataFrame(cm, index=['benign', 'defacement', 'phishing', 'malware'],
                     columns=['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, fmt="d", cmap='Blues')
plt.title('Random Forest - Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Feature Importances - Random Forest
feat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind="barh", figsize=(10, 6))
plt.title("Random Forest - Feature Importances")
plt.show()

"""LightGBM Classifier"""

# LightGBM Classifier
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Create and train the LightGBM model
lgb = LGBMClassifier(objective='multiclass', boosting_type='gbdt', n_jobs=-1, verbose=-1, random_state=42)
LGB_C = lgb.fit(X_train, y_train)

# Predict on test set
y_pred_lgb = LGB_C.predict(X_test)

# Find unique classes to generate correct target_names
unique_classes = np.unique(y_test)
print("Unique classes:", unique_classes)

# Define class names matching the number of unique classes
# Example mapping (adjust based on your actual dataset)
class_names = ['benign', 'defacement', 'phishing', 'malware', 'spam']  # Update as needed

# Ensure number of names matches number of unique classes
if len(unique_classes) == len(class_names):
    print(classification_report(y_test, y_pred_lgb, target_names=class_names))
else:
    print(f"Mismatch in class count: {len(unique_classes)} classes vs {len(class_names)} names.")
    print("Using default class indices instead.")
    print(classification_report(y_test, y_pred_lgb))

# Accuracy score
score = accuracy_score(y_test, y_pred_lgb)
print("LightGBM Accuracy: %0.3f" % score)

# LightGBM Classifier
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Create and train the LightGBM model
lgb = LGBMClassifier(objective='multiclass', boosting_type='gbdt', n_jobs=-1, verbose=-1, random_state=42)
LGB_C = lgb.fit(X_train, y_train)

# Predict on test set
y_pred_lgb = LGB_C.predict(X_test)

# Find unique classes to generate correct target_names
unique_classes = np.unique(y_test)
print("Unique classes:", unique_classes)

# Define class names matching the number of unique classes
# Update based on your actual dataset
class_names = ['benign', 'defacement', 'phishing', 'malware', 'spam']

# Ensure number of names matches number of unique classes
if len(unique_classes) == len(class_names):
    print(classification_report(y_test, y_pred_lgb, target_names=class_names))
else:
    print(f"Mismatch in class count: {len(unique_classes)} classes vs {len(class_names)} names.")
    print("Using default class indices instead.")
    print(classification_report(y_test, y_pred_lgb))

# Accuracy score
score = accuracy_score(y_test, y_pred_lgb)
print("LightGBM Accuracy: %0.3f" % score)

# Feature Importances - LightGBM
feat_importances = pd.Series(LGB_C.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind="barh", figsize=(10, 6))
plt.title("LightGBM - Feature Importances")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""# XGBoost Classifier"""

xgb_c = xgb.XGBClassifier(n_estimators=100, random_state=5)
xgb_c.fit(X_train, y_train)
y_pred_x = xgb_c.predict(X_test)
print(classification_report(y_test, y_pred_x, target_names=['benign', 'defacement', 'phishing', 'malware']))

score = accuracy_score(y_test, y_pred_x)
print("XGBoost Accuracy:   %0.3f" % score)

# Confusion Matrix for XGBoost
cm = confusion_matrix(y_test, y_pred_x)
cm_df = pd.DataFrame(cm, index=['benign', 'defacement', 'phishing', 'malware'],
                     columns=['benign', 'defacement', 'phishing', 'malware'])
plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, fmt="d", cmap='Blues')
plt.title('XGBoost - Confusion Matrix')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

# Feature Importances - XGBoost
feat_importances = pd.Series(xgb_c.feature_importances_, index=X_train.columns)
feat_importances.sort_values().plot(kind="barh", figsize=(10, 6))
plt.title("XGBoost - Feature Importances")
plt.show()

import warnings

# Suppress all warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

def main(url):
    status = []

    status.append(having_ip_address(url))
    status.append(abnormal_url(url))
    status.append(count_dot(url))
    status.append(count_www(url))
    status.append(count_atrate(url))
    status.append(no_of_dir(url))
    status.append(no_of_embed(url))

    status.append(shortening_service(url))
    status.append(count_https(url))
    status.append(count_http(url))

    status.append(count_per(url))
    status.append(count_ques(url))
    status.append(count_hyphen(url))
    status.append(count_equal(url))

    status.append(url_length(url))
    status.append(hostname_length(url))
    status.append(suspicious_words(url))
    status.append(digit_count(url))
    status.append(letter_count(url))
    status.append(fd_length(url))

    tld = get_tld(url, fail_silently=True)
    status.append(tld_length(tld))

    return status

def get_prediction_from_url_rf(test_url):
    features_test = main(test_url)
    features_test = np.array(features_test).reshape((1, -1))  # Ensure 2D input

    pred = rf.predict(features_test)  # Use Random Forest Classifier (rf)

    if int(pred[0]) == 0:
        return "SAFE"
    elif int(pred[0]) == 1:
        return "DEFACEMENT"
    elif int(pred[0]) == 2:
        return "PHISHING"
    elif int(pred[0]) == 3:
        return "MALWARE"

# List of URLs to test
urls = ['titaniumcorporate.co.za', 'en.wikipedia.org/wiki/North_Dakota']

# Running prediction using Random Forest Classifier
for url in urls:
    print(f"URL: {url} -> Prediction: {get_prediction_from_url_rf(url)}")

# List of URLs to test
urls = ['mp3raid.com/music/krizz_kaliko.html', 'http://www.pashminaonline.com/pure-pashminas']

# Running prediction using Random Forest Classifier
for url in urls:
    print(f"URL: {url} -> Prediction: {get_prediction_from_url_rf(url)}")